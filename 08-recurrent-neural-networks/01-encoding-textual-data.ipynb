{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enconding Natural Data\n",
    "\n",
    "One of the primary challeges in natural language processing is properly encoding the text so that mathematical models such as neural networks can extract meaning from the text, and process it into some kind of result. The field has evolved quite a lot over time, and we're going to briefly present a few of the once-common strategies for language encoding then spend some time talking about the current favorite method: embeddings.\n",
    "\n",
    "## Rules Based Systems\n",
    "\n",
    "At first there were rules based systems. Like many other systems at the time, these were \"expert systems\" designed with extensive help from linguists and grammatician. These systems would look at a sentnce and try to identify critical aspects of a sentence (the subject, object, key verb...), label the words into types (adjective, noun, verb adverb), and map their function (which adjectives describe which nouns?).\n",
    "\n",
    "These expert systems could produce some data structure representing the sentence or document, and then these structures could be used for some custom built purpose (grammar check, chat bot...)\n",
    "\n",
    "Unlike ML systems, these systems didn't have any real purpose for statistical based learning, and as a result the data structures didn't need to be strictly numerically based, nor did they have to explicitly encode individual words or longer sentences into pure numeric formats. \n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "Once statistical learning methods began advancing, NLP researchers needed pure numeric encodings that could meaningfully represent, words, sentences, or other strings of text. Some simple strategies like the classic one-hot-encoding were fleetingly popular â€” but the need for a vector the size of the entire vocabulary just to represent a single word was prohibitively costly for all but the simplest use cases.\n",
    "\n",
    "The \"Bag of Words\" encoding came next. Instead of representing single words at a time, this strategy reduces an entire string of text into a vector. Again the vector is the lenght of the vocabulary, but instead of one-hot the individual values are the number of times that word appears in a the text.\n",
    "\n",
    "For example, say our vocabulary is only 5 words:\n",
    "\n",
    "Hello, live, work, to, friend\n",
    "\n",
    "Each of these words is represented by a position in a vector, and we loop through the text to create the vector for our text. \n",
    "\n",
    "The sentence, \"Hello friend\" becomes the vector `[1, 0, 0, 0, 1]`  \n",
    "\n",
    "\"Live to work\" and \"work to live\" both become the vector `[0, 1, 1, 1, 0]`\n",
    "\n",
    "For some simple tasks such an encoding can work reasonably well especially if the input texts are always short. For example, mapping Tweets to a binary positive/negative sentiment system. But such a simple system clearly discards much of the semantic meaning of the sentence by completely ignoring the order of the words. It also struggles with words like \"live\" that have multiple possible meanings (\"The new feature is going live tomorrow.\" vs \"She is going to live!\")\n",
    "\n",
    "Such an encoding can indeed be used with a standard ANN. That said, none of the state of the art research is proceeding down this path.\n",
    "\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF, or Term Frequencey Inverse Document Frequency is a way to turn an individual word into a numeric value rather than process a series of words the way Bag of Words would. The TF-IDF value is a representation of how common a word is within a particular document as well as how common the word is accross all documents in an entire corpus or dataset. We won't be using it, but you can view the [mathematical details on Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) if you're curious\n",
    "\n",
    "While this is useful in some information retrieval contexts for many natural language tasks such as machine translation and sentiment analysis, information about a words commonality isn't especially helpful. Researchers needed a way to create a numeric representation of individual words that could somehow capture the semantic meaning of those words...\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "In 2013, a team of researchers at Google published two papers describing Word2Vec, a neural network that transformed individual words into vectors that could represent the word's semantic meaning. Word2Vec formed a strong foundation for reseach, and ultimately led to the creation of a now-standard tool: Word Embedding layers. \n",
    "\n",
    "Word embedding layers provide a generic interface for the first layer of any neural network that wants to process one word at a time as input. Their function is simple: Embedding layers act as a lookup table mapping a word from our vocabulary into a dense vector of a chosen length representing that word.\n",
    "\n",
    "The values associated with each position in the word-vector are learned during back-propagation just as the weights in a Dense layer would be, but don't require an entire matrix multiply as each word maps to a single vector of the matrix representing the entire vocabulary. This difference saves computational time, but learns through backpropagation very similarly to a Dense layer. \n",
    "\n",
    "Sometimes we may use a pre-trained network (such as Word2Vec) to create the word embeddings. Although it is now quite common to train an embedding layer as part of the network, which allows the word embeddings to learn patterns that are specific to the task at hand. \n",
    "\n",
    "Regardless, the result is a lookup that maps words to vectors and (if it works as expected) words that are closely related in the dataset result in vectors that are near each other in vector space. When it REALLY works, these embedding vectors can be thought of as a rich set of features extracted from the word.\n",
    "\n",
    "Analysis of the embeddings themselves can be done. For example computing the cosine similarity between two words in the resulting vector space frequently reveals that semantically related words like cat and kitty are close to each other in the resulting vector space, as explained here [https://medium.com/@hari4om/word-embedding-d816f643140](https://medium.com/@hari4om/word-embedding-d816f643140)\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*sAJdxEsDjsPMioHyzlN3_A.png)\n",
    "\n",
    "> image from linked reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
