{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and Sequence Generation\n",
    "\n",
    "In this notebook we're going to explore two changes to RNN we built in the previous notebook. \n",
    "\n",
    "1. The use of LSTM layers instead of SimpleRNN layers.\n",
    "2. Generating sequences instead of generating a single prediction as output.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "A Long Short Term Memory layer is an extension of the RNN idea, and one that is designed to do 2 things:\n",
    "\n",
    "1. Give the hidden state more flexibillity in how it updates.\n",
    "2. Provide an efficent route for backpropagation, similar to skip layers in CNNs\n",
    "\n",
    "Here is a diagram from one of the readings [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/):\n",
    "\n",
    "![](https://colah.github.io/images/post-covers/lstm.png)\n",
    "\n",
    "What you see here are the addition of several \"gates\" as well as an additional output from the layer compared to a simple RNN. Gates act like other hidden layers and have their own learned weights. \n",
    "\n",
    "The two states are called \"cell state\" (the top line) and \"hidden state\" the bottom line. Both serve a similar purpose to the RNN hidden state, but the cell state's progression doesn't involve an activation function over time which helps avoid the vanishing gradient problem that simple RNN's often suffer from.\n",
    "\n",
    "The gates are typically referred to as follows:\n",
    "\n",
    "**Forget gate**: The first gate uses a sigmoid activation to produce values between 0 and 1, those values are then pointwise multiplied with the incoming hidden state. This allows the cell state to \"forget\" irrelevant context when the activations are near 0. This gate is designed such that it can move the values in the cell state closer to zero.\n",
    "\n",
    "**Input and \"gate\" gates**: The second gate is a multiply and involves the sigmoid and tanh activations. The multiply gate allows the sigmoid to scales the tanh output. The result is a value between -1 and 1 which gets pointwise added to the cell state (after the forget is applied). This allows the cell state to incrementally learn new information and store it into the context. \n",
    "\n",
    "The sigmoid is called the \"input\" gate and the tanh is called the \"gate gate\" but they both work together to decide how much the cell state learns from the new input.\n",
    "\n",
    "**Output gate**: Finally, the last sigmoid is the output gate, and it scales the output next output and hidden state values, but not the cell state.\n",
    "\n",
    "## Sequence to sequence\n",
    "\n",
    "The next change we'll make is to allow multiple outputs over time, allowing us to generate text output from text input rather than a single output for classification or regression.\n",
    "\n",
    "![](https://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "This requires a few changes.\n",
    "\n",
    "1. First, the training data. We'll be using a setup where our inputs and labels will be from the exact same text, but at every timestep the label will be the word directly following the current word. \n",
    "2. Second we have to enable each timestep to produce a prediction.\n",
    "3. We have to map those numeric predictions to a word.\n",
    "4. We have to enable the network to stop somehow, we'll be allowing the network to generate as one of the \"words\" a \"stop\" token which when predicted will cause the network to stop.\n",
    "\n",
    "Note that this strategy works for generating text both character by character and word by word. We will be performing a character by character training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[20 49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45\n",
      "  3 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45\n",
      " 41 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51\n",
      "  8  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12\n",
      "  2 39 55 61], shape=(100,), dtype=int64) \n",
      " b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "\n",
      "tf.Tensor(\n",
      "[49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45  3\n",
      " 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45 41\n",
      " 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51  8\n",
      "  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12  2\n",
      " 39 55 61  3], shape=(100,), dtype=int64) \n",
      " b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# load the tiny_shakespear dataset, 40,000 lines of text from various Shakspear plays\n",
    "# but only the \"train\" subset.\n",
    "dataset = tfds.load(name='tiny_shakespeare')['train']\n",
    "\n",
    "# Split the dataset from each line being a string to each line being an array of characters\n",
    "# in UTF-8 encoding\n",
    "dataset = dataset.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
    "\n",
    "# Extract all the uniqe charcaters to form the vocabulary\n",
    "vocabulary = sorted(set(next(iter(dataset)).numpy()))\n",
    "\n",
    "# We're creating two functions to swap between characters and their int lookup value\n",
    "ids_from_chars = StringLookup(\n",
    "    vocabulary=list(vocabulary)\n",
    ")\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True\n",
    ")\n",
    "\n",
    "# For training at each step we're asking the model to predict the next character\n",
    "# based on the current state + current character.\n",
    "dataset = dataset.map(lambda x: (ids_from_chars(x[:-1]), ids_from_chars(x[1:])))\n",
    "\n",
    "# Unbatch kind of flattens the data.\n",
    "# We're sort of implying that any line can flow fluidly into another line\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Now we're chopping the flat data into sequences of 100 characters each\n",
    "seq_len = 100\n",
    "dataset = dataset.batch(seq_len, drop_remainder = True)\n",
    "\n",
    "# We can see that \"next_char\" is just the \"cur_char\" shifted one position.\n",
    "# Because they are already encoded as UTF code points, they have a resaonblly efficent \n",
    "# numeric representation.\n",
    "for current_char, next_char in dataset.take(1):\n",
    "    print(current_char, '\\n', b''.join(chars_from_ids(current_char).numpy()))\n",
    "    print()\n",
    "    print(next_char, '\\n', b''.join(chars_from_ids(next_char).numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're going to shuffle and batch it. Standard when working with Tensorflow's Dataset class\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Some things to note: \n",
    "\n",
    "1. We're using an LSTM rather than a simple RNN layer.\n",
    "2. It has a larger relative internal representation.\n",
    "3. The embedding dimension is similarly larger than last time.\n",
    "4. The output dimension is the size of the vocab (every unique character in the training data!)\n",
    "\n",
    "Also, we're actually subclassing the model class. This allows us to have a bit more control over when and how the layers talk to each other, and how we deal with state. Code slightly adapted from TF docs [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[33 34 19 ... 59  3 58]\n",
      " [ 3 48 55 ... 14  2  2]\n",
      " [48 55 54 ...  2 15 54]\n",
      " ...\n",
      " [ 3 48 45 ... 55 44  3]\n",
      " [63 45 58 ... 62 45 54]\n",
      " [ 2 37 48 ...  3 53 41]], shape=(64, 100), dtype=int64)\n",
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"text_generator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17152     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  68675     \n",
      "=================================================================\n",
      "Total params: 5,332,803\n",
      "Trainable params: 5,332,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TextGeneratorModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Second\n",
    "        self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True)\n",
    "\n",
    "        # Dense last.\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, h_states=None, c_states=None, return_state=False, training=False):\n",
    "        # Transform the input using the embedding layer\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        # If there is no incoming state from an earlier timestep\n",
    "        # use the default initial state behavior\n",
    "        if h_states is None:\n",
    "            h_states, c_states = self.lstm.get_initial_state(x)\n",
    "        \n",
    "        # Transfrom the embedding using lstm\n",
    "        # This is transformed output, transformed hidden state, and _ is hidden cell state\n",
    "        x, h_states, c_states = self.lstm(x, initial_state=[h_states, c_states], training=training)\n",
    "\n",
    "        # Pass the transformed x into the dense layer\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        # Only return state when asked\n",
    "        if return_state:\n",
    "            return x, h_states, c_states\n",
    "        else: \n",
    "            return x\n",
    "\n",
    "\n",
    "# embedding output == 256, rnn units = 1024\n",
    "model = TextGeneratorModel(len(ids_from_chars.get_vocabulary()), 256, 1024)\n",
    "\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'STER:\\nVouchsafe to wear this ring.\\n\\nLADY ANNE:\\nTo take is not to give.\\n\\nGLOUCESTER:\\nLook, how this r'\n",
      "\n",
      "Predictions:\n",
      " b\"pohMgQBAvJiBaAFLi&Io&VIqJx3o\\n.g$'vSf!ayAo'xHACecx'XVwG&ftqYvSw3IQHU&'tGmomG?TGXHKxfJ[UNK]R-uC.dchMoTF\"\n"
     ]
    }
   ],
   "source": [
    "# Lets see what happens on the untrained network...\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"Input:\\n\", b''.join(chars_from_ids(input_example_batch[0]).numpy()))\n",
    "print()\n",
    "print(\"Predictions:\\n\", b''.join(chars_from_ids(sampled_indices).numpy()))\n",
    "# Of course... a lot of garbage :)\n",
    "# But that's just because it's not trained... hopefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.2058873\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "156/156 [==============================] - 313s 2s/step - loss: 3.6787\n",
      "Epoch 2/20\n",
      "156/156 [==============================] - 316s 2s/step - loss: 3.1506\n",
      "Epoch 3/20\n",
      "156/156 [==============================] - 309s 2s/step - loss: 2.7136\n",
      "Epoch 4/20\n",
      "156/156 [==============================] - 306s 2s/step - loss: 2.5071\n",
      "Epoch 5/20\n",
      "156/156 [==============================] - 306s 2s/step - loss: 2.4055\n",
      "Epoch 6/20\n",
      "156/156 [==============================] - 308s 2s/step - loss: 2.3352\n",
      "Epoch 7/20\n",
      "156/156 [==============================] - 308s 2s/step - loss: 2.2793\n",
      "Epoch 8/20\n",
      "156/156 [==============================] - 306s 2s/step - loss: 2.2297\n",
      "Epoch 9/20\n",
      "156/156 [==============================] - 321s 2s/step - loss: 2.1832\n",
      "Epoch 10/20\n",
      "156/156 [==============================] - 307s 2s/step - loss: 2.1427\n",
      "Epoch 11/20\n",
      "156/156 [==============================] - 317s 2s/step - loss: 2.1084\n",
      "Epoch 12/20\n",
      "156/156 [==============================] - 307s 2s/step - loss: 2.0678\n",
      "Epoch 13/20\n",
      "156/156 [==============================] - 307s 2s/step - loss: 2.0371\n",
      "Epoch 14/20\n",
      "156/156 [==============================] - 310s 2s/step - loss: 2.0048\n",
      "Epoch 15/20\n",
      "156/156 [==============================] - 304s 2s/step - loss: 1.9780\n",
      "Epoch 16/20\n",
      "156/156 [==============================] - 310s 2s/step - loss: 1.9475\n",
      "Epoch 17/20\n",
      "156/156 [==============================] - 309s 2s/step - loss: 1.9215\n",
      "Epoch 18/20\n",
      "156/156 [==============================] - 310s 2s/step - loss: 1.8974\n",
      "Epoch 19/20\n",
      "156/156 [==============================] - 310s 2s/step - loss: 1.8758\n",
      "Epoch 20/20\n",
      "156/156 [==============================] - 310s 2s/step - loss: 1.8486\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset, \n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text With The Model\n",
    "\n",
    "Using a model to generate text is done by using the outputs from the dense layer as a sampling distribution. At each step we run the model once, maintaining the state, sampling from the prediction, and then using our sampled character as input for the next timestep. Tensorflow's documentation has a wonderful visual and class to help us do this more easily, so we've stolen both. From [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "\n",
    "![](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
    "\n",
    "This strategy generates text of a fixed length, it's also common to encode a STOP token during training and generate text until our model samples that STOP token naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature=temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices = skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, h_states=None, c_states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "        predicted_logits, h_states, c_states =  self.model(inputs=input_ids, h_states=h_states, c_states=c_states, return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, h_states, c_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Thar ow, Gowill your blooke me dind aty:\n",
      "Thele nok sand-'ther in thoungy, me, mearey,\n",
      "Was pay'st will; no, due for I pion my lovy?\n",
      "\n",
      "GLOUGESTE:\n",
      "\n",
      "LUTIN:\n",
      "Th, lid a tread, forrwith sance;\n",
      "Of the dess arded tay, thaik thee,\n",
      "Affach us the tome promone vain his nows,\n",
      "And the pronst of thy ades in theprew.\n",
      "\n",
      "IVGENRIIS:\n",
      "He vinst My I fo do here, we the int in it\n",
      "Mond the be inom; Ancelf this dalinges corvers\n",
      "Then atarmy, I so goncelily stanter,\n",
      "What thou and you see be fors'd flaling me ot.\n",
      "\n",
      "AMMERIO:\n",
      "I riser, wallfst'd, I briged a,\n",
      "Thy pemaremy, las! 'scast-treer's harainby, sires,\n",
      "It me I'll me proren, ona troathon's mengrat'\n",
      "Then will I fow wherd', as mose thou, spuck,\n",
      "Bod beth it whon infurdectire unstloust; mo\n",
      "not wirm,, to bat sighther, ay shees onand; take ant\n",
      "dayoud the poiltt though,\n",
      "A toorblowe visted that me sobend all rave?\n",
      "\n",
      "Firttrcange:\n",
      "And, the friad thy crancurow.\n",
      "\n",
      "ustinge: a s'llased was whel' de thou magh, dy\n",
      "Ralist, thown, the cicrse in of andsevinges-\n",
      "\n",
      "Emod that I da, were you \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.812701940536499\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "start = time.time()\n",
    "h_states = None\n",
    "c_states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, h_states, c_states = one_step_model.generate_one_step(next_char, h_states=h_states, c_states=c_states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "Training was very long.\n",
    "\n",
    "The result is... not amazing, but there are some things to be impressed with:\n",
    "\n",
    "1. This architecture is *dead simple.* More compelex implementations of the same idea can do much better. \n",
    "2. Even thought training was slow, we didn't train *that* much in the grand scheme of things.\n",
    "3. We generated this *character by character* and it was able to pick up on some very interesting structure.\n",
    "4. We might not be using the best optimizer settings, that's something to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
